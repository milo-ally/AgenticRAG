{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2957c3ca",
   "metadata": {},
   "source": [
    "# <center> 模拟前端"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de09ab56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "步骤1: 上传文件到RAG服务...\n",
      "============================================================\n",
      "✅ 文件上传成功！\n",
      "   文件名: test.pdf\n",
      "   知识库ID: 2fb820d3-5cb9-46d5-9126-fa5b7412c36c\n",
      "   文件大小: 2228535 bytes\n",
      "   分块数量: 44\n",
      "   Token统计: {'vectorbase_usage': 7864, 'query_usage': 0, 'total_usage': 7864}\n",
      "\n",
      "============================================================\n",
      "步骤2: 通过文件名查询知识库...\n",
      "============================================================\n",
      "查询请求: 请查询文件名为'test.pdf'的知识库中关于Transformer的内容\n",
      "\n",
      "我将帮您查询文件名为'test.pdf'的知识库中关于Transformer的内容。\n",
      "\n",
      "[工具调用]\n",
      "  工具名称: semantic_search\n",
      "  调用参数: {}\n",
      "\n",
      "[工具返回结果]\n",
      "  状态码: 200\n",
      "  知识库ID: 2fb820d3-5cb9-46d5-9126-fa5b7412c36c\n",
      "  文件名: test.pdf\n",
      "  内容预览: Transformer\n",
      "基本背景\n",
      " Transformer由论文《Attention is All You Need》提出，现在是谷歌云TPU推荐的参考模型。论文相关的\n",
      "Tensorflow的代码可以从GitHub获取，其作为Tensor2Tensor包的一部分。哈佛的NLP团队也实现了一个基于\n",
      "PyTorch的版本，并注释该论文。\n",
      " Transformer解决了RNN无法并行计算， 长距离信息遗...\n",
      "根据查询结果，在'test.pdf'文件中关于Transformer的内容如下：\n",
      "\n",
      "## Transformer概述\n",
      "\n",
      "Transformer是由论文《Attention is All You Need》提出的模型，现在是谷歌云TPU推荐的参考模型。该模型解决了传统RNN的两个主要问题：\n",
      "1. **无法并行计算**\n",
      "2. **长距离信息遗忘**\n",
      "\n",
      "## 核心特点\n",
      "Transformer彻底摒弃了RNN和CNN结构，转而使用**注意力机制**来解决问题。\n",
      "\n",
      "## 整体结构\n",
      "Transformer采用**Encoder-Decoder结构**：\n",
      "\n",
      "### Encoder（编码器）\n",
      "负责输入序列特征编码，包含以下处理步骤：\n",
      "1. **前置处理**：\n",
      "   - 词嵌入（token-embedding）\n",
      "   - 位置编码（position-embedding）\n",
      "\n",
      "2. **单层结构（TransformerBlock）**：\n",
      "   - 多头自注意力层（MultiHeadAttention）：捕捉输入序列内部token依赖关系\n",
      "   - 第一次Add & Norm：残差连接 + 层归一化，保留原始信息并稳定训练过程\n",
      "   - 前馈神经网络（FFN）：独立非线性变换token向量\n",
      "\n",
      "## 工作流程\n",
      "\n",
      "### 第一步：嵌入（Embedding）\n",
      "分为两个子步骤：\n",
      "\n",
      "1. **词嵌入（token-embedding）**：\n",
      "   - 可以通过Word2Vec、One-hot、Glove等算法预训练得到\n",
      "   - 也可以在Transformer中训练得到\n",
      "\n",
      "2. **位置编码（position-embedding）**：\n",
      "   - 使用特定公式计算，使模型能够：\n",
      "     - 处理比训练集更长的句子\n",
      "     - 容易计算出相对位置关系\n",
      "\n",
      "这个知识库提供了Transformer模型的基本背景、整体结构和工作流程的详细说明。\n",
      "\n",
      "[Token使用统计]\n",
      "  输入Token: 2763\n",
      "  输出Token: 459\n",
      "  总计Token: 3222\n"
     ]
    }
   ],
   "source": [
    "# ========== 完整流程：上传文件 -> 通过文件名查询 ==========\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "rag_base_url = \"http://localhost:5000\"  # RAG服务地址 (记得先启动redis!)\n",
    "base_url = \"http://localhost:8000\"      # ChatAgent服务地址\n",
    "file_path = \"./test.pdf\"\n",
    "\n",
    "# ========== 步骤1: 上传文件到RAG服务，获取文件信息 ==========\n",
    "def upload_file_and_get_info(rag_service_base_url: str, file_path: str, extract_images: bool = False):\n",
    "    try:\n",
    "        upload_url = f\"{rag_service_base_url}/upload-pdf?extract_images={extract_images}\"\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            upload_response = requests.post(upload_url, files={\"file\": f})\n",
    "        \n",
    "        if upload_response.status_code != 200:\n",
    "            print(f\"上传失败，状态码: {upload_response.status_code}\")\n",
    "            print(f\"错误信息: {upload_response.text}\")\n",
    "            return None\n",
    "        \n",
    "        data = upload_response.json()[\"data\"]\n",
    "        return {\n",
    "            \"filename\": data[\"filename\"],  # 文件名\n",
    "            \"kb_id\": data[\"knowledge_base_id\"],  # 知识库ID\n",
    "            \"file_size\": data[\"file_size\"],\n",
    "            \"chunk_count\": data[\"chunk_count\"],\n",
    "            \"token_stats\": data[\"token_stats\"]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"上传文件错误：{e}\")\n",
    "        return None\n",
    "\n",
    "# 上传文件并获取信息\n",
    "print(\"=\" * 60)\n",
    "print(\"步骤1: 上传文件到RAG服务...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "file_info = upload_file_and_get_info(\n",
    "    rag_service_base_url=rag_base_url, \n",
    "    file_path=file_path, \n",
    "    extract_images=False\n",
    ")\n",
    "\n",
    "if not file_info:\n",
    "    print(\"❌ 文件上传失败，请检查文件路径和RAG服务状态\")\n",
    "else:\n",
    "    print(\"✅ 文件上传成功！\")\n",
    "    print(f\"   文件名: {file_info['filename']}\")\n",
    "    print(f\"   知识库ID: {file_info['kb_id']}\")\n",
    "    print(f\"   文件大小: {file_info['file_size']} bytes\")\n",
    "    print(f\"   分块数量: {file_info['chunk_count']}\")\n",
    "    print(f\"   Token统计: {file_info['token_stats']}\")\n",
    "    \n",
    "    # ========== 步骤2: 使用文件名进行查询 ==========\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"步骤2: 通过文件名查询知识库...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    url = \"http://localhost:8000/v1/chat/completions\"\n",
    "    FIXED_SESSION_ID = \"my_test_session_filename_query\"\n",
    "    \n",
    "    # 在查询中直接使用文件名，Agent会自动通过文件名找到对应的知识库\n",
    "    request_data = {\n",
    "        \"user_id\": \"test_user3\",\n",
    "        \"session_id\": FIXED_SESSION_ID,\n",
    "        \"query\": f\"请查询文件名为'{file_info['filename']}'的知识库中关于Transformer的内容\"\n",
    "    }\n",
    "    \n",
    "    print(f\"查询请求: {request_data['query']}\\n\")\n",
    "    \n",
    "    response = requests.post(\n",
    "        url,\n",
    "        json=request_data,\n",
    "        stream=True,\n",
    "        headers={\"Content-Type\": \"application/json\"}\n",
    "    )\n",
    "    \n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            line_str = line.decode(\"utf-8\").strip()\n",
    "            if line_str.startswith(\"data: \"):\n",
    "                json_str = line_str[6:]\n",
    "                try:\n",
    "                    resp = json.loads(json_str)\n",
    "                    \n",
    "                    # 1. AI逐字符流式输出: 前端要实时渲染为markdown\n",
    "                    if resp[\"type\"] == \"content\":\n",
    "                        print(resp[\"content\"], end=\"\", flush=True)\n",
    "                    \n",
    "                    # 2. AI正在使用工具：需要标记为特殊类型\n",
    "                    elif resp[\"type\"] == \"tool_call\":\n",
    "                        print(f\"\\n\\n[工具调用]\")\n",
    "                        print(f\"  工具名称: {resp['content']['tool_name']}\")\n",
    "                        print(f\"  调用参数: {resp['content']['tool_args']}\")\n",
    "                    \n",
    "                    # 3. AI工具返回结果: 需要溯源验证\n",
    "                    elif resp[\"type\"] == \"tool_result\":\n",
    "                        try:\n",
    "                            tool_result = json.loads(resp[\"content\"][\"called_tool_content\"])\n",
    "                            print(f\"\\n[工具返回结果]\")\n",
    "                            print(f\"  状态码: {tool_result['metadata'].get('status_code', 'N/A')}\")\n",
    "                            print(f\"  知识库ID: {tool_result['metadata'].get('kb_id', 'N/A')}\")\n",
    "                            print(f\"  文件名: {tool_result['metadata'].get('filename', 'N/A')}\")\n",
    "                            print(f\"  内容预览: {tool_result['content'][:200]}...\")\n",
    "                        except:\n",
    "                            print(f\"\\n[工具返回结果] {resp['content']}\")\n",
    "                    \n",
    "                    # 4. 单轮对话token消耗: 需要统计当前用户点数\n",
    "                    elif resp[\"type\"] == \"usage\":\n",
    "                        print(f\"\\n\\n[Token使用统计]\")\n",
    "                        print(f\"  输入Token: {resp['content']['input_tokens']}\")\n",
    "                        print(f\"  输出Token: {resp['content']['output_tokens']}\")\n",
    "                        print(f\"  总计Token: {resp['content']['total_tokens']}\")\n",
    "                except Exception as e:\n",
    "                    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0cd5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在查找文件名 'test.pdf' 对应的知识库ID...\n",
      "✅ 找到知识库ID: 2fb820d3-5cb9-46d5-9126-fa5b7412c36c\n",
      "\n",
      "使用获取到的kb_id进行查询...\n",
      "查询请求: 请查询知识库2fb820d3-5cb9-46d5-9126-fa5b7412c36c中关于Transformer的内容\n",
      "\n",
      "我需要使用语义搜索工具来查询您指定的知识库。从您提供的信息来看，您希望查询知识库ID为\"2fb820d3-5cb9-46d5-9126-fa5b7412c36c\"中关于Transformer的内容。\n",
      "\n",
      "让我为您进行查询：\n",
      "[工具调用] semantic_search\n",
      "\n",
      "[工具返回结果]\n",
      "根据查询结果，我从知识库\"2fb820d3-5cb9-46d5-9126-fa5b7412c36c\"中找到了关于Transformer的详细内容。以下是主要信息：\n",
      "\n",
      "## Transformer概述\n",
      "\n",
      "Transformer是由论文《Attention is All You Need》提出的模型，现在是谷歌云TPU推荐的参考模型。它解决了传统RNN无法并行计算和长距离信息遗忘的问题，彻底摒弃了RNN和CNN结构，转而使用注意力机制。\n",
      "\n",
      "## Transformer整体结构与工作流程\n",
      "\n",
      "### 1. 整体结构\n",
      "Transformer采用Encoder-Decoder结构：\n",
      "- **Encoder（编码器）**：负责输入序列特征编码\n",
      "- **Decoder（解码器）**：负责输出序列生成\n",
      "\n",
      "### 2. 工作流程\n",
      "\n",
      "#### 第一步：嵌入（Embedding）\n",
      "包含两个子步骤：\n",
      "1. **词嵌入（token-embedding）**：可以通过Word2Vec、One-hot、Glove等算法预训练得到，也可以在Transformer中训练得到\n",
      "2. **位置编码（position-embedding）**：使用特定公式计算，使模型能够：\n",
      "   - 处理比训练集更长的句子\n",
      "   - 容易计算出相对位置\n",
      "\n",
      "#### Encoder单层结构（TransformerBlock）\n",
      "按顺序执行以下操作：\n",
      "1. **多头自注意力层（MultiHeadAttention）**：捕捉输入序列内部token之间的依赖关系\n",
      "2. **第一次Add & Norm**：残差连接 + 层归一化，目的是保留原始信息并稳定训练过程\n",
      "3. **前馈神经网络（FFN）**：对token向量进行独立的非线性变换\n",
      "\n",
      "## 位置编码公式\n",
      "位置编码使用正弦和余弦函数组合的公式，这种设计使得模型能够：\n",
      "1. 处理任意长度的序列（即使超过训练时的最大长度）\n",
      "2. 更容易学习相对位置关系\n",
      "\n",
      "这个知识库提供了Transformer的基础理论、架构设计和工作原理的详细说明，对于理解现代深度学习中的注意力机制非常有帮助。\n",
      "\n",
      "[Token使用统计] {'input_tokens': 2873, 'output_tokens': 596, 'total_tokens': 3469}\n"
     ]
    }
   ],
   "source": [
    "# ========== 方式2: 先获取文件名对应的kb_id，然后查询 ==========\n",
    "# 如果你已经上传了文件，可以通过文件名获取kb_id\n",
    "\n",
    "def get_kb_id_by_filename(rag_service_base_url: str, filename: str):\n",
    "    \"\"\"通过文件名获取知识库ID\"\"\"\n",
    "    try:\n",
    "        get_kb_id_url = f\"{rag_service_base_url}/get-kb-id-by-filename\"\n",
    "        response = requests.get(\n",
    "            get_kb_id_url,\n",
    "            params={\"filename\": filename},\n",
    "            timeout=30\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            return response.json()[\"data\"][\"knowledge_base_id\"]\n",
    "        else:\n",
    "            print(f\"错误：{response.text}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"错误：{e}\")\n",
    "        return None\n",
    "\n",
    "# 示例：通过文件名获取kb_id（前提是文件已经上传）\n",
    "filename = \"test.pdf\"\n",
    "print(f\"正在查找文件名 '{filename}' 对应的知识库ID...\")\n",
    "\n",
    "kb_id= get_kb_id_by_filename(rag_base_url, filename)\n",
    "\n",
    "if kb_id:\n",
    "    print(f\"✅ 找到知识库ID: {kb_id}\")\n",
    "    \n",
    "    # 然后可以使用kb_id进行查询\n",
    "    url = \"http://localhost:8000/v1/chat/completions\"\n",
    "    request_data = {\n",
    "        \"user_id\": \"test_user3\",\n",
    "        \"session_id\": \"my_test_session_kb_id_query\",\n",
    "        \"query\": f\"请查询知识库{kb_id}中关于Transformer的内容\"\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n使用获取到的kb_id进行查询...\")\n",
    "    print(f\"查询请求: {request_data['query']}\\n\")\n",
    "    \n",
    "    response = requests.post(\n",
    "        url,\n",
    "        json=request_data,\n",
    "        stream=True,\n",
    "        headers={\"Content-Type\": \"application/json\"}\n",
    "    )\n",
    "    \n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            line_str = line.decode(\"utf-8\").strip()\n",
    "            if line_str.startswith(\"data: \"):\n",
    "                json_str = line_str[6:]\n",
    "                try:\n",
    "                    resp = json.loads(json_str)\n",
    "                    \n",
    "                    if resp[\"type\"] == \"content\":\n",
    "                        print(resp[\"content\"], end=\"\", flush=True)\n",
    "                    elif resp[\"type\"] == \"tool_call\":\n",
    "                        print(f\"\\n[工具调用] {resp['content']['tool_name']}\")\n",
    "                    elif resp[\"type\"] == \"tool_result\":\n",
    "                        print(f\"\\n[工具返回结果]\")\n",
    "                    elif resp[\"type\"] == \"usage\":\n",
    "                        print(f\"\\n\\n[Token使用统计] {resp['content']}\")\n",
    "                except:\n",
    "                    pass\n",
    "else:\n",
    "    print(f\"❌ 无法找到文件名 '{filename}' 对应的知识库ID\")\n",
    "    print(\"提示：请先运行上面的代码上传文件到RAG服务\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bf8f6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件解析成功! 知识库ID: 8b70d6e6-4df0-4f61-bd88-bc461ed6ebb5\n",
      "我看到您想了解知识库`8b70d6e6-4df0-4f61-bd88-bc461ed6ebb5`中关于Transformer的内容。让我为您调用语义搜索工具：{'type': 'tool_call', 'content': {'tool_name': 'semantic_search', 'tool_call_id': 'call_00_d3fAE5vYRZVNnUsZg5Fhfn8a', 'tool_args': {}}}\n",
      "工具返回结果:  {'called_tool_name': 'semantic_search', 'called_tool_content': '{\"content\": \"Transformer\\\\n基本背景\\\\n Transformer由论文《Attention is All You Need》提出，现在是谷歌云TPU推荐的参考模型。论文相关的\\\\nTensorflow的代码可以从GitHub获取，其作为Tensor2Tensor包的一部分。哈佛的NLP团队也实现了一个基于\\\\nPyTorch的版本，并注释该论文。\\\\n Transformer解决了RNN无法并行计算， 长距离信息遗忘的问题, 并且彻底摒弃RNN, CNN结构来计算，转而使用\\\\nAttention 注意力机制来解决问题。\\\\n1. Transformer 整体结构与工作流程\\\\n1.1 Transformer的整体结构\\\\n首先介绍Transformer的整体结构，传统的 Transformer是 Encoder-Decoder 结构。\\\\n下图是Transformer的总体结构 (重要) :\\\\n\\\\n可以看到 Transformer由Encoder(左)和Decoder(右)两个部分 组成。\\\\nEncoder（编码器）：输入序列特征编码\\\\n前置处理：词嵌入token-embedding 和 位置编码 position-embedding\\\\n单层结构（TransformerBlock）（注意: 顺序执行）：\\\\n多头自注意力(MultiHeadAttention) 层（捕捉输入序列内部 token 依赖）\\\\n第一次 Add & Norm（残差连接 + 层归一化, 目的: 保留原始信息， 稳定训练过程。）\\\\n前馈神经网络（FFN）：独立非线性变换 token 向量\\\\n\\\\n1.2 Transformer的工作流程\\\\nTransformer的工作流程如下(重要)：\\\\n第一步 嵌入 （Embedding）\\\\n分为 词嵌入 和 位置编码 两个子步骤。\\\\n词嵌入( token-embedding ) 如何得到\\\\n单词的 Embedding 有很多种方式可以获取，例如可以采用 Word2Vec 、 One-hot 、 Glove 等算法预训练\\\\n得到，也可以 在 Transformer 中训练 得到。\\\\n位置编码 ( position-embedding ) \\\\n其中， 表示单词在句子中的位置， 表示 的维度， 表示偶数的维度， 表示奇数维度。\\\\n即 。( 注意: 就是位置编码 position-embedding ，且 ) \\\\n使用这种公式计算 PE 有以下的好处：\\\\n1. 使位置向量能够适应比训练集里面所有句子更长的句子。\\\\n假设训练集里面最长的句子长度是20个token, 突然来了一个token长度为为21的句子, 则使用公式计算的方\\\\n法可以计算出第 21 位的 Embedding。 )\\\\n2. 可以让模型容易地计算出相对位置。\", \"metadata\": {\"token_usage\": {\"vectorbase_usage\": 7864, \"query_usage\": 1, \"total_usage\": 7865}, \"status_code\": 200, \"kb_id\": \"8b70d6e6-4df0-4f61-bd88-bc461ed6ebb5\", \"filename\": null, \"top_k\": 3}}', 'called_tool_metadata': \"{'token_usage': {'vectorbase_usage': 7864, 'query_usage': 1, 'total_usage': 7865}, 'status_code': 200, 'kb_id': '8b70d6e6-4df0-4f61-bd88-bc461ed6ebb5', 'filename': None, 'top_k': 3}\"}\n",
      "根据从知识库`8b70d6e6-4df0-4f61-bd88-bc461ed6ebb5`中检索到的关于Transformer的内容，我来为您详细介绍：\n",
      "\n",
      "## Transformer 详细介绍\n",
      "\n",
      "### 基本背景\n",
      "Transformer由论文《Attention is All You Need》提出，现已成为谷歌云TPU推荐的参考模型。该模型有多个实现版本：\n",
      "- Tensorflow版本：可从GitHub获取，作为Tensor2Tensor包的一部分\n",
      "- PyTorch版本：由哈佛NLP团队实现并注释\n",
      "\n",
      "### 核心创新\n",
      "Transformer主要解决了传统RNN的两个关键问题：\n",
      "1. **无法并行计算** - RNN的序列依赖性限制了计算并行性\n",
      "2. **长距离信息遗忘** - RNN在处理长序列时容易遗忘早期信息\n",
      "\n",
      "Transformer彻底摒弃了RNN和CNN结构，转而使用**注意力机制**来解决问题。\n",
      "\n",
      "### 整体架构\n",
      "Transformer采用经典的**Encoder-Decoder**结构：\n",
      "\n",
      "#### **Encoder（编码器）**\n",
      "- **输入处理**：\n",
      "  - 词嵌入(token-embedding)\n",
      "  - 位置编码(position-embedding)\n",
      "- **单层结构（TransformerBlock）**：\n",
      "  1. 多头自注意力层(MultiHeadAttention) - 捕捉输入序列内部token依赖关系\n",
      "  2. 第一次Add & Norm - 残差连接 + 层归一化（保留原始信息，稳定训练）\n",
      "  3. 前馈神经网络(FFN) - 独立非线性变换token向量\n",
      "\n",
      "### 工作流程详解\n",
      "\n",
      "#### **第一步：嵌入（Embedding）**\n",
      "分为两个子步骤：\n",
      "\n",
      "1. **词嵌入(token-embedding)**\n",
      "   - 获取方式多样：Word2Vec、One-hot、Glove等预训练算法\n",
      "   - 也可以在Transformer训练过程中学习得到\n",
      "\n",
      "2. **位置编码(position-embedding)**\n",
      "   - 使用特定公式计算位置向量\n",
      "   - 公式：`PE(pos,2i) = sin(pos/10000^(2i/d_model))` 和 `PE(pos,2i+1) = cos(pos/10000^(2i/d_model))`\n",
      "   - 其中：`pos`表示单词在句子中的位置，`i`表示维度索引\n",
      "\n",
      "#### **位置编码的优势**\n",
      "1. **适应更长句子**：即使训练集中最长句子只有20个token，也能计算第21个位置的编码\n",
      "2. **便于计算相对位置**：模型能够轻松理解token之间的相对位置关系\n",
      "\n",
      "### 技术特点\n",
      "- **完全注意力驱动**：不使用循环或卷积操作\n",
      "- **并行处理能力**：所有位置可同时计算\n",
      "- **长距离依赖捕捉**：通过自注意力机制有效处理长序列\n",
      "- **位置感知**：通过位置编码保持序列顺序信息\n",
      "\n",
      "Transformer的这种架构设计使其在机器翻译、文本生成等序列到序列任务中表现出色，并为后来的BERT、GPT等模型奠定了基础。{'type': 'usage', 'content': {'input_tokens': 7615, 'output_tokens': 775, 'total_tokens': 8390}}\n"
     ]
    }
   ],
   "source": [
    "# ========== 方式3: 传统方式 - 使用kb_id查询（对比） ==========\n",
    "# 这是原来的方式，需要记住kb_id\n",
    "\n",
    "rag_base_url = \"http://localhost:5000\"\n",
    "base_url = \"http://localhost:8000\"\n",
    "file_path = \"./test.pdf\"\n",
    "\n",
    "def get_kb_id(rag_service_base_url: str, file_path: str, extract_images: bool = False): \n",
    "    \"\"\"上传文件并返回kb_id\"\"\"\n",
    "    try: \n",
    "        upload_url = f\"{rag_service_base_url}/upload-pdf?extract_images={extract_images}\"\n",
    "        with open(file_path, \"rb\") as f: \n",
    "            upload_response = requests.post(upload_url, files={\"file\": f})\n",
    "        \n",
    "        if upload_response.status_code != 200:\n",
    "            return {\"kb_id\": \"NAN\", \"token_stats\": 0}\n",
    "        \n",
    "        data = upload_response.json()[\"data\"]\n",
    "        return {\"kb_id\": data[\"knowledge_base_id\"], \"token_stats\": data[\"token_stats\"]}\n",
    "    except Exception as e: \n",
    "        print(f\"错误：{e}\")\n",
    "        return {\"kb_id\": \"NAN\", \"token_stats\": 0}\n",
    "\n",
    "# 上传文件获取kb_id\n",
    "response = get_kb_id(rag_service_base_url=rag_base_url, extract_images=False, file_path=file_path)\n",
    "kb_id = response[\"kb_id\"]\n",
    "\n",
    "if kb_id != \"NAN\":\n",
    "    print(f\"文件解析成功! 知识库ID: {kb_id}\")\n",
    "    \n",
    "    url = \"http://localhost:8000/v1/chat/completions\"\n",
    "    FIXED_SESSION_ID = \"my_test_session_01\"\n",
    "    \n",
    "    request_data = {\n",
    "        \"user_id\": \"test_user3\",\n",
    "        \"session_id\": FIXED_SESSION_ID,\n",
    "        \"query\": f\"你好, 请介绍一下知识库{kb_id}的关于Transformer的内容\"\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        url,\n",
    "        json=request_data,\n",
    "        stream=True,\n",
    "        headers={\"Content-Type\": \"application/json\"}\n",
    "    )\n",
    "    \n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            line_str = line.decode(\"utf-8\").strip()\n",
    "            if line_str.startswith(\"data: \"):\n",
    "                json_str = line_str[6:]\n",
    "                try:\n",
    "                    resp = json.loads(json_str)\n",
    "                    \n",
    "                    if resp[\"type\"] == \"content\":\n",
    "                        print(resp[\"content\"], end=\"\", flush=True)\n",
    "                    elif resp[\"type\"] == \"tool_call\":\n",
    "                        print(resp)\n",
    "                    elif resp[\"type\"] == \"tool_result\":\n",
    "                        print(\"工具返回结果: \", resp[\"content\"])\n",
    "                    elif resp[\"type\"] == \"usage\":\n",
    "                        print(resp)\n",
    "                except:\n",
    "                    pass\n",
    "else:\n",
    "    print(\"文件上传失败\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
