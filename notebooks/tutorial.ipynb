{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "250a3b55",
   "metadata": {},
   "source": [
    "## 1. Agentçš„æ„å»ºæ€è·¯ä»¥åŠæµå¼è¾“å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e9dc9566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade pip -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# pip install pypdfium2\n",
    "# pip install Pillow\n",
    "# pip install dashscope\n",
    "# pip install rapidocr-onnxruntime\n",
    "# pip install pypdf -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# pip install pydantic fastapi -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# pip install aiosqlite -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# pip install -U langchain langchain-openai langchain-community \"langgraph-checkpoint-sqlite>=1.1.0\" -i https://pypi.tuna.tsinghua.edu.cn/simple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "78c2270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random \n",
    "import uuid\n",
    "from pathlib import Path \n",
    "import asyncio\n",
    "import aiosqlite\n",
    "\n",
    "def patch_aiosqlite_is_alive():\n",
    "    \"\"\"è¡¥ä¸ï¼šä¸ºaiosqlite.Connectionæ·»åŠ is_aliveæ–¹æ³•\"\"\"\n",
    "    if not hasattr(aiosqlite.Connection, 'is_alive'):\n",
    "        def is_alive(self):\n",
    "            \"\"\"Return True if the underlying sqlite3 connection exists.\"\"\"\n",
    "            return self._connection is not None\n",
    "        aiosqlite.Connection.is_alive = is_alive\n",
    "\n",
    "patch_aiosqlite_is_alive()\n",
    "\n",
    "from langchain_openai.chat_models import ChatOpenAI # å¤§è„‘\n",
    "from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver # è®°å¿†\n",
    "from langchain.tools import tool # å·¥å…·\n",
    "from langchain.agents import create_agent\n",
    "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage, SystemMessage\n",
    "from langgraph.types import StateSnapshot, PregelTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c9f6ead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. åˆå§‹åŒ–å¤§è„‘\n",
    "model = ChatOpenAI(\n",
    "    model=\"deepseek-chat\",\n",
    "    base_url=\"https://api.deepseek.com\", \n",
    "    api_key=\"sk-14a5fe04297040c28b4f91c5dfa938a1\", \n",
    ")\n",
    "\n",
    "# 2. åˆå§‹åŒ–è®°å¿†ä¸è®°å¿†ç®¡ç†\n",
    "user_id = \"user_001\" # ç”¨æˆ·id\n",
    "session_id = str(uuid.uuid4()) # å•è½®ä¼šè¯id\n",
    "memory_dir = Path(\"./memories\") # å­˜æ”¾æ‰€ç”¨ç”¨æˆ·ä¼šè¯æ–‡ä»¶çš„æ ¹ç›®å½•\n",
    "user_memory_dir = memory_dir / user_id\n",
    "user_memory_dir.mkdir(parents=True, exist_ok=True)\n",
    "memory_uri = user_memory_dir / f\"session_{session_id}.db\"#  ç”± user_id å’Œ session_id å†³å®š\n",
    "\n",
    "# 3. åˆå§‹åŒ–å·¥å…·\n",
    "@tool\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"\n",
    "    Get weather information for a given location.\n",
    "    Args:\n",
    "        location: The city to query weather for (e.g. \"Beijing\")\n",
    "    Returns:\n",
    "        A string of mock weather information\n",
    "    \"\"\"\n",
    "    weather = random.choice([\"Sunny\", \"Cloudy\", \"Rainy\"])\n",
    "    temp = random.randint(5, 30)\n",
    "    return f\"Weather in {location}: {weather}, Temperature: {temp}Â°C\"\n",
    "\n",
    "tools = [get_weather]\n",
    "\n",
    "# 4. ç³»ç»Ÿæç¤ºè¯\n",
    "system_prompt = \"ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„æ™ºèƒ½åŠ©æ‰‹\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0ee8fdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\34943\\AppData\\Local\\Temp\\ipykernel_21476\\1468383473.py:7: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "  class AgentConfig(BaseModel):\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import BaseTool\n",
    "from langchain.chat_models import BaseChatModel \n",
    "from pydantic import BaseModel, Field \n",
    "from typing import List, Union\n",
    "from pathlib import Path\n",
    "\n",
    "class AgentConfig(BaseModel):  \n",
    "    model: BaseChatModel = Field(..., description=\"Such as ChatOpenAI\")\n",
    "    tools: List[BaseTool] = Field(default_factory=list, description=\"List of callable langchain tools\")\n",
    "    memory_uri: Union[str, Path] = Field(..., description=\"The Path to SQLite Database to memory storage\")\n",
    "    system_prompt: str = Field(..., description=\"system_prompt\")\n",
    "    \n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "    \n",
    "    @property\n",
    "    def memory_uri_str(self) -> str:\n",
    "        return str(self.memory_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "16631b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Set, Dict, Any, AsyncGenerator\n",
    "\n",
    "class AgentService:\n",
    "    def __init__(self, agent_config: AgentConfig):\n",
    "        self.agent_config = agent_config  \n",
    "    \n",
    "    async def call_agent(\n",
    "        self,\n",
    "        query: str,  # user query\n",
    "        session_id: str,  # 1 session-id <-> 1 memory\n",
    "    ) -> AsyncGenerator[Dict[str, Any], None]:\n",
    "\n",
    "        tool_call_ids: Set[str] = set()  # ç»Ÿè®¡è°ƒç”¨çš„å·¥å…·id\n",
    "        total_input_tokens: int = 0  # total_input_tokens (including function call message)\n",
    "        total_output_tokens: int = 0  # total_output_tokens\n",
    "        usage: Dict[str, int] | None = None  # token usage\n",
    "        \n",
    "        # extract agent's config from `AgentConfig`\n",
    "        model = self.agent_config.model\n",
    "        tools = self.agent_config.tools\n",
    "        memory_uri = self.agent_config.memory_uri_str  \n",
    "        system_prompt = self.agent_config.system_prompt\n",
    "        \n",
    "        async with AsyncSqliteSaver.from_conn_string(memory_uri) as checkpointer:\n",
    "            await checkpointer.setup()\n",
    "            agent = create_agent(\n",
    "                model=model, \n",
    "                tools=tools, \n",
    "                checkpointer=checkpointer, \n",
    "                system_prompt=system_prompt\n",
    "            )\n",
    "            config = {\n",
    "                \"configurable\": {\n",
    "                    \"thread_id\": session_id\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            async for chunk in agent.astream({\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\", \n",
    "                        \"content\": query\n",
    "                    }\n",
    "                ]\n",
    "            }, config=config, stream_mode=\"messages\"):\n",
    "                message = chunk[0]\n",
    "                state = await agent.aget_state(config=config)\n",
    "                \n",
    "                # starting token analyzation\n",
    "                tasks = state.tasks\n",
    "                for task in tasks:\n",
    "                    if task.result is not None: \n",
    "                        for msg in task.result[\"messages\"]:\n",
    "                            if isinstance(msg, AIMessage) and hasattr(msg, 'usage_metadata'):\n",
    "                                usage_metadata = msg.usage_metadata\n",
    "                                current_input = usage_metadata.get(\"input_tokens\", 0)\n",
    "                                current_output = usage_metadata.get(\"output_tokens\", 0)\n",
    "                                if current_input not in [0, total_input_tokens]:\n",
    "                                    total_input_tokens += current_input\n",
    "                                if current_output not in [0, total_output_tokens]:\n",
    "                                    total_output_tokens += current_output\n",
    "                \n",
    "                # function call message\n",
    "                if isinstance(message, AIMessage) and message.tool_calls:\n",
    "                    for tool_call in message.tool_calls: \n",
    "                        tool_call_id = tool_call.get(\"id\")\n",
    "                        tool_name = tool_call.get(\"name\")\n",
    "                        if tool_name and tool_call_id and tool_call_id not in tool_call_ids:\n",
    "                            tool_info = f\"{tool_name}\"\n",
    "                            yield {\n",
    "                                \"type\": \"tool_call\", \n",
    "                                \"content\": {\n",
    "                                    \"tool_name\": tool_name, \n",
    "                                    \"tool_call_id\": tool_call_id, \n",
    "                                }\n",
    "                            }\n",
    "                            tool_call_ids.add(tool_call_id)\n",
    "            \n",
    "                # completion message\n",
    "                elif isinstance(message, AIMessage) and message.content: \n",
    "                    for char in message.content: \n",
    "                        yield {\n",
    "                            \"type\": \"content\", \n",
    "                            \"content\": char\n",
    "                        }\n",
    "                        \n",
    "            \n",
    "                # tool response message\n",
    "                elif isinstance(message, ToolMessage):\n",
    "                    called_tool_name = message.name \n",
    "                    called_tool_content = message.content\n",
    "                    yield {\n",
    "                        \"type\": \"tool_result\", \n",
    "                        \"content\": {\n",
    "                            \"called_tool_name\": called_tool_name, \n",
    "                            \"called_tool_content\": called_tool_content\n",
    "                        }\n",
    "                    }\n",
    "            \n",
    "            # yield token statisitics\n",
    "            total_tokens = total_input_tokens + total_output_tokens\n",
    "            usage = {\n",
    "                \"input_tokens\": total_input_tokens, \n",
    "                \"output_tokens\": total_output_tokens, \n",
    "                \"total_tokens\": total_tokens\n",
    "            }\n",
    "            yield {\n",
    "                \"type\": \"usage\",\n",
    "                \"content\": usage\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "32b07882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆ‘æ¥å¸®æ‚¨æŸ¥çœ‹æ·±åœ³å’Œæ­å·çš„å¤©æ°”æƒ…å†µã€‚{'tool_name': 'get_weather', 'tool_call_id': 'call_00_mXEtdZZImPAX86nOSwvNv2ka'}\n",
      "{'called_tool_name': 'get_weather', 'called_tool_content': 'Weather in æ·±åœ³: Sunny, Temperature: 13Â°C'}\n",
      "{'tool_name': 'get_weather', 'tool_call_id': 'call_00_RX8JYVQqAe4sokbZ4YtVIG8e'}\n",
      "{'called_tool_name': 'get_weather', 'called_tool_content': 'Weather in æ­å·: Sunny, Temperature: 28Â°C'}\n",
      "æ ¹æ®æŸ¥è¯¢ç»“æœï¼š\n",
      "\n",
      "**æ·±åœ³å¤©æ°”**ï¼šæ™´å¤©ï¼Œæ¸©åº¦13Â°C\n",
      "\n",
      "**æ­å·å¤©æ°”**ï¼šæ™´å¤©ï¼Œæ¸©åº¦28Â°C\n",
      "\n",
      "ä¸¤ä¸ªåŸå¸‚ä»Šå¤©éƒ½æ˜¯æ™´å¤©ï¼Œä½†æ¸©åº¦å·®å¼‚è¾ƒå¤§ã€‚æ·±åœ³ç›¸å¯¹å‡‰çˆ½ï¼ˆ13Â°Cï¼‰ï¼Œè€Œæ­å·æ¯”è¾ƒæ¸©æš–ï¼ˆ28Â°Cï¼‰ã€‚å¦‚æœæ‚¨éœ€è¦å‡ºè¡Œï¼Œå»ºè®®æ ¹æ®ç›®çš„åœ°æ¸©åº¦å‡†å¤‡åˆé€‚çš„è¡£ç‰©ã€‚è¾“å…¥Token: 1283\n",
      "è¾“å‡ºToken: 166\n",
      "æ€»Token: 1449\n"
     ]
    }
   ],
   "source": [
    "# 1ï¼šåˆå§‹åŒ–ç”¨æˆ·å’Œä¼šè¯ä¿¡æ¯\n",
    "user_id = \"user_001\"\n",
    "session_id = str(uuid.uuid4())\n",
    "\n",
    "# 2. åˆå§‹åŒ–agent\n",
    "agent_config = AgentConfig(\n",
    "\tmodel=model, \n",
    "\ttools=tools, \n",
    "\tmemory_uri=memory_uri, \n",
    "\tsystem_prompt=system_prompt\n",
    ")\n",
    "agent_service = AgentService(agent_config=agent_config)\n",
    "# 3. æµå¼è¾“å‡ºå“åº”\n",
    "query=\"ä½ å¥½, è¯·å¸®æˆ‘çœ‹çœ‹æ·±åœ³å’Œæ­å·çš„å¤©æ°”ã€‚\"\n",
    "generator = agent_service.call_agent(\n",
    "\tquery=query,\n",
    "\tsession_id=session_id\n",
    ")\n",
    "async for chunk in generator:\n",
    "\tif chunk[\"type\"] == \"content\":\n",
    "\t\tprint(chunk[\"content\"], end=\"\", flush=True)\n",
    "\telif chunk[\"type\"] == \"tool_call\":\n",
    "\t\tprint(chunk[\"content\"])\n",
    "\telif chunk[\"type\"] == \"tool_result\":\n",
    "\t\tprint(chunk[\"content\"])\n",
    "\telif chunk[\"type\"] == \"usage\":\n",
    "\t\tprint(f\"è¾“å…¥Token: {chunk['content']['input_tokens']}\")\n",
    "\t\tprint(f\"è¾“å‡ºToken: {chunk['content']['output_tokens']}\")\n",
    "\t\tprint(f\"æ€»Token: {chunk['content']['total_tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddff32e9",
   "metadata": {},
   "source": [
    "## 2. åŸºäºRAGçš„å¤šæ¨¡æ€PDFè§£æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "475e4a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nå‚æ•°ç»„åˆ\\tæ‰§è¡Œè¡Œä¸º\\nby_id=False\\tå¿½ç•¥ vector_db_idï¼Œæ‰¹é‡åˆ é™¤æ‰€æœ‰ UUID æ ¼å¼å‘é‡åº“æ–‡ä»¶å¤¹\\nby_id=False, vector_db_id=\"xxx\"\\tä»æ‰¹é‡åˆ é™¤æ‰€æœ‰ï¼Œä¼ å…¥çš„ ID è¢«å¿½ç•¥\\nby_id=True\\tå¿…é¡»ä¼  vector_db_idï¼Œå¦åˆ™æŠ¥é”™\\nby_id=True, vector_db_id=\"xxx\"\\tä»…åˆ é™¤æŒ‡å®š ID çš„å‘é‡åº“\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents.base import Blob\n",
    "from langchain_community.document_loaders.parsers.pdf import PyPDFium2Parser\n",
    "from langchain_community.vectorstores import FAISS  # pip install faiss-cpu\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter \n",
    "from langchain_core.documents import Document\n",
    "from typing import List, Dict, Any, Optional, Callable\n",
    "import uuid  \n",
    "import tiktoken  # pip install tiktoken \n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "\n",
    "class VectorDataBase(FAISS): \n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        embedding_function, \n",
    "        index, \n",
    "        docstore, \n",
    "        index_to_docstore_id, \n",
    "        relevance_score_fn: Optional[Callable[[float], float]] = None, \n",
    "        normalize_L2: bool = False, \n",
    "        distance_strategy: DistanceStrategy = DistanceStrategy.EUCLIDEAN_DISTANCE, \n",
    "        id: Optional[str] = None  \n",
    "    ):\n",
    "        # è°ƒç”¨çˆ¶ç±»FAISSçš„åˆå§‹åŒ–ï¼ˆä¸¥æ ¼åŒ¹é…å‚æ•°é¡ºåºå’Œåç§°ï¼‰\n",
    "        super().__init__(\n",
    "            embedding_function=embedding_function,\n",
    "            index=index,\n",
    "            docstore=docstore,\n",
    "            index_to_docstore_id=index_to_docstore_id,\n",
    "            relevance_score_fn=relevance_score_fn,\n",
    "            normalize_L2=normalize_L2,\n",
    "            distance_strategy=distance_strategy\n",
    "        )\n",
    "\n",
    "        # åˆå§‹åŒ–è‡ªå®šä¹‰idï¼ˆä¸ä¼ åˆ™è‡ªåŠ¨ç”ŸæˆUUIDï¼‰\n",
    "        self.id = id or str(uuid.uuid4())\n",
    "\n",
    "    def save_local(self, folder_path: str, index_name: str = \"index\") -> None:\n",
    "        super().save_local(folder_path, index_name)\n",
    "        path = Path(folder_path)\n",
    "        path.mkdir(exist_ok=True, parents=True)\n",
    "        with open(path / f\"{index_name}_id.pkl\", \"wb\") as f: \n",
    "            pickle.dump({\"id\": self.id}, f)\n",
    "\n",
    "    @classmethod\n",
    "    def load_local(\n",
    "        cls, \n",
    "        folder_path: str, \n",
    "        embeddings, \n",
    "        index_name: str = \"index\", \n",
    "        *, \n",
    "        allow_dangerous_deserialization: bool = False, \n",
    "        **kwargs\n",
    "    ) -> \"VectorDataBase\":\n",
    "        \n",
    "        # 1. åŠ è½½åŸç”ŸFAISSå®ä¾‹\n",
    "        base_instance = super().load_local(\n",
    "            folder_path=folder_path,\n",
    "            embeddings=embeddings,\n",
    "            index_name=index_name,\n",
    "            allow_dangerous_deserialization=allow_dangerous_deserialization,** kwargs\n",
    "        )\n",
    "        \n",
    "        # 2. åŠ è½½è‡ªå®šä¹‰id\n",
    "        path = Path(folder_path)\n",
    "        id = None\n",
    "        try:\n",
    "            with open(path / f\"{index_name}_id.pkl\", \"rb\") as f: \n",
    "                id_data = pickle.load(f) \n",
    "                id = id_data.get(\"id\")\n",
    "        except FileNotFoundError:\n",
    "            id = str(uuid.uuid4())\n",
    "        \n",
    "        # 3. è½¬æ¢ä¸ºVectorDataBaseå®ä¾‹ï¼ˆå¤ç”¨åŸç”Ÿå±æ€§ï¼Œæ·»åŠ è‡ªå®šä¹‰idï¼‰\n",
    "        instance = cls(\n",
    "            embedding_function=base_instance.embedding_function,\n",
    "            index=base_instance.index,\n",
    "            docstore=base_instance.docstore,\n",
    "            index_to_docstore_id=base_instance.index_to_docstore_id,\n",
    "            relevance_score_fn=base_instance.override_relevance_score_fn,\n",
    "            normalize_L2=base_instance._normalize_L2,\n",
    "            distance_strategy=base_instance.distance_strategy,\n",
    "            id=id  # æ³¨å…¥è‡ªå®šä¹‰id\n",
    "        )\n",
    "        return instance\n",
    "\n",
    "    @classmethod\n",
    "    def from_documents(\n",
    "        cls,\n",
    "        documents: list[Document],\n",
    "        embedding,\n",
    "        id: Optional[str] = None,  # æ–°å¢ï¼šè‡ªå®šä¹‰idå‚æ•°ï¼ˆä½ çš„å˜é‡åï¼‰\n",
    "        **kwargs: Any,\n",
    "    ) -> \"VectorDataBase\":\n",
    "        \n",
    "        # 1. è°ƒç”¨çˆ¶ç±»çš„from_documentsåˆ›å»ºåŸç”ŸFAISSå®ä¾‹\n",
    "        base_faiss = super().from_documents(documents, embedding, **kwargs)\n",
    "        \n",
    "        # 2. è½¬æ¢ä¸ºVectorDataBaseå®ä¾‹å¹¶æ³¨å…¥è‡ªå®šä¹‰id\n",
    "        vector_db = cls(\n",
    "            embedding_function=base_faiss.embedding_function,\n",
    "            index=base_faiss.index,\n",
    "            docstore=base_faiss.docstore,\n",
    "            index_to_docstore_id=base_faiss.index_to_docstore_id,\n",
    "            relevance_score_fn=base_faiss.override_relevance_score_fn,\n",
    "            normalize_L2=base_faiss._normalize_L2,\n",
    "            distance_strategy=base_faiss.distance_strategy,\n",
    "            id=id  \n",
    "        )\n",
    "        return vector_db\n",
    "\n",
    "class RAGTool:\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_api_key: str,\n",
    "        embedding_model: str = \"text-embedding-v3\",\n",
    "        chunk_size: int = 500,\n",
    "        chunk_overlap: int = 200,\n",
    "        extract_image: bool = False\n",
    "    ):\n",
    "        self.embedding_api_key = embedding_api_key\n",
    "        self.embedding_model = embedding_model\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.extract_image = extract_image\n",
    "        \n",
    "        self.embedding = DashScopeEmbeddings(\n",
    "            dashscope_api_key=self.embedding_api_key, \n",
    "            model=self.embedding_model\n",
    "        )\n",
    "        \n",
    "        self.pdf_chunks: List[Dict] = []\n",
    "        self.vector_db: Optional[VectorDataBase] = None  \n",
    "        self.file_info: Dict[str, Any] = {}\n",
    "        self.token_encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        self.vectorbase_usage = 0\n",
    "        self.query_usage = 0\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        return len(self.token_encoder.encode(text)) if text else 0\n",
    "    \n",
    "    def process_pdf(self, file_content: bytes, filename: str) -> List[Dict]:\n",
    "        try:\n",
    "            # 1. æ„å»ºBlob\n",
    "            blob = Blob.from_data(\n",
    "                data=file_content,\n",
    "                path=filename\n",
    "            )\n",
    "            \n",
    "            # 2. PDFè§£æ\n",
    "            parser = PyPDFium2Parser(\n",
    "                extract_images=self.extract_image,\n",
    "                images_inner_format=\"html-img\"\n",
    "            )\n",
    "            documents = list(parser.parse(blob))\n",
    "            \n",
    "            # 3. æ‹¼æ¥å®Œæ•´æ–‡æœ¬\n",
    "            full_content = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "            \n",
    "            # 4. æ–‡æœ¬åˆ†å—\n",
    "            splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=self.chunk_size,\n",
    "                chunk_overlap=self.chunk_overlap,\n",
    "                separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "            )\n",
    "            all_chunks = splitter.split_text(full_content)\n",
    "            \n",
    "            # 5. æ ¼å¼åŒ–åˆ†å—ç»“æœ\n",
    "            chunks = []\n",
    "            self.vectorbase_usage = 0\n",
    "            total_chunks = len(all_chunks)\n",
    "            for i, chunk_text in enumerate(all_chunks):\n",
    "                clean_text = chunk_text.strip()\n",
    "                if not clean_text:\n",
    "                    continue\n",
    "                chunk_token = self.count_tokens(clean_text)\n",
    "                self.vectorbase_usage += chunk_token\n",
    "                chunks.append({\n",
    "                    \"id\": str(uuid.uuid4()),\n",
    "                    \"content\": clean_text,\n",
    "                    \"metadata\": {\n",
    "                        \"filename\": filename,\n",
    "                        \"chunk_index\": i,\n",
    "                        \"total_chunks\": total_chunks,\n",
    "                        \"token\": chunk_token\n",
    "                    }\n",
    "                })\n",
    "            \n",
    "            self.pdf_chunks = chunks\n",
    "            self.file_info = {\n",
    "                \"filename\": filename,\n",
    "                \"chunk_count\": len(chunks),\n",
    "                \"file_size\": len(file_content),\n",
    "                \"extract_image\": self.extract_image,\n",
    "                \"vectorbase_usage\": self.vectorbase_usage\n",
    "            }\n",
    "            \n",
    "            return chunks\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to chunk PDF: {str(e)}\")\n",
    "    \n",
    "    def build_vector_db(self, vector_db_id: Optional[str] = None) -> VectorDataBase:\n",
    "        if not self.pdf_chunks:\n",
    "            raise RuntimeError(\"Please process PDF with process_pdf first before building vector database\")\n",
    "        \n",
    "        try:\n",
    "            langchain_docs = []\n",
    "            for chunk in self.pdf_chunks:\n",
    "                langchain_doc = Document(\n",
    "                    page_content=chunk[\"content\"],\n",
    "                    metadata={\n",
    "                        \"id\": chunk[\"id\"],\n",
    "                        \"filename\": chunk[\"metadata\"][\"filename\"],\n",
    "                        \"chunk_index\": chunk[\"metadata\"][\"chunk_index\"],\n",
    "                        \"total_chunks\": chunk[\"metadata\"][\"total_chunks\"]\n",
    "                    }\n",
    "                )\n",
    "                langchain_docs.append(langchain_doc)\n",
    "            \n",
    "            self.vector_db = VectorDataBase.from_documents(\n",
    "                documents=langchain_docs,\n",
    "                embedding=self.embedding,\n",
    "                id=vector_db_id\n",
    "            )\n",
    "            self.file_info[\"vector_db_id\"] = self.vector_db.id # æ–°å¢ä¸€ä¸ªé”®å€¼å¯¹\n",
    "            \n",
    "            return self.vector_db\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to build FAISS vector database: {str(e)}\")\n",
    "    \n",
    "    def semantic_search(self, query: str, top_k: int = 3) -> List[Dict]:\n",
    "        if not self.vector_db:\n",
    "            raise RuntimeError(\"Please build vector database first before performing search\")\n",
    "        \n",
    "        try:\n",
    "            query_token = self.count_tokens(query)\n",
    "            self.query_usage += query_token\n",
    "            \n",
    "            results_with_score = self.vector_db.similarity_search_with_score(query, k=top_k)\n",
    "            \n",
    "            formatted_results = []\n",
    "            for doc, score in results_with_score:\n",
    "                formatted_results.append({\n",
    "                    \"id\": doc.metadata[\"id\"],\n",
    "                    \"content\": doc.page_content,\n",
    "                    \"metadata\": doc.metadata,\n",
    "                    \"similarity_score\": round(score, 4),\n",
    "                    \"vector_db_id\": self.vector_db.id  # è¿”å›ä½ çš„è‡ªå®šä¹‰id\n",
    "                })\n",
    "            \n",
    "            return formatted_results\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Semantic search failed: {str(e)}\")\n",
    "    \n",
    "    def save_vector_db(self, folder_path: str, index_name: str = \"index\") -> None:\n",
    "        if not self.vector_db:\n",
    "            raise RuntimeError(\"Please build vector database first before saving\")\n",
    "        self.vector_db.save_local(folder_path, index_name)\n",
    "        print(f\"Vector database (id: {self.vector_db.id}) saved to: {folder_path}\")\n",
    "    \n",
    "    def load_vector_db(self, id: str) -> VectorDataBase:\n",
    "        try:\n",
    "            folder_path = f\"./{id}\"\n",
    "            index_name = id \n",
    "            self.vector_db = VectorDataBase.load_local(\n",
    "                folder_path=folder_path,\n",
    "                index_name=index_name,\n",
    "                embeddings=self.embedding, \n",
    "                allow_dangerous_deserialization=True # ç”Ÿäº§ç¯å¢ƒéœ€è°¨æ…\n",
    "            )\n",
    "            if self.vector_db.id != id:\n",
    "                raise RuntimeError(f\"Unmatched: {self.vector_db.id} != {id}\")\n",
    "            self.file_info[\"vector_db_id\"] = id \n",
    "            return self.vector_db\n",
    "        except FileNotFoundError: \n",
    "            raise RuntimeError(f\"VectorBase-{id} not found in {folder_path}\")\n",
    "        except Exception as e: \n",
    "            raise RuntimeError(f\"Failed\")\n",
    "    \n",
    "    def get_token_stats(self) -> Dict:\n",
    "        return {\n",
    "            \"vectorbase_usage\": self.vectorbase_usage,\n",
    "            \"query_usage\": self.query_usage,\n",
    "            \"total_usage\": self.vectorbase_usage + self.query_usage\n",
    "        }\n",
    "    \n",
    "    def get_file_info(self) -> Dict:\n",
    "        return self.file_info\n",
    "    \n",
    "    def clear(self):\n",
    "        self.pdf_chunks = []\n",
    "        self.vector_db = None\n",
    "        self.file_info = {}\n",
    "        self.vectorbase_usage = 0\n",
    "        self.query_usage = 0\n",
    "\n",
    "    def clear_disk(self, by_id: bool = False, vector_db_id: Optional[str] = None) -> None:\n",
    "        try:\n",
    "            # by_id=Trueï¼šæŒ‰IDåˆ é™¤ï¼ˆä¼ IDåˆ æŒ‡å®šï¼Œä¸ä¼ IDæŠ¥é”™ï¼‰\n",
    "            if by_id:\n",
    "                if not vector_db_id:\n",
    "                    raise RuntimeError(\"vector_db_id is required when by_id=True\")\n",
    "                target_path = Path(f\"./{vector_db_id}\")\n",
    "                if not target_path.exists():\n",
    "                    raise RuntimeError(f\"Vector database {vector_db_id} not found on disk\")\n",
    "                if target_path.is_dir():\n",
    "                    shutil.rmtree(target_path)\n",
    "                    print(f\"Successfully deleted vector database {vector_db_id} from {target_path}\")\n",
    "                elif target_path.is_file():\n",
    "                    target_path.unlink()\n",
    "                    print(f\"Successfully deleted vector database file {vector_db_id}\")\n",
    "            # by_id=Falseï¼šå¼ºåˆ¶æ‰¹é‡åˆ é™¤ï¼ˆå¿½ç•¥vector_db_idï¼Œç›´æ¥åˆ æ‰€æœ‰ï¼‰\n",
    "            else:\n",
    "                deleted_count = 0\n",
    "                current_dir = Path(\".\")\n",
    "                for item in current_dir.iterdir():\n",
    "                    if item.is_dir():\n",
    "                        try:\n",
    "                            uuid.UUID(item.name)\n",
    "                            shutil.rmtree(item)\n",
    "                            deleted_count += 1\n",
    "                            print(f\"Deleted vector database: {item}\")\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "                if deleted_count == 0:\n",
    "                    print(\"No vector databases found on disk to delete\")\n",
    "                else:\n",
    "                    print(f\"Successfully deleted {deleted_count} vector databases\")\n",
    "        except PermissionError:\n",
    "            raise RuntimeError(\"Permission denied: cannot delete vector database files\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to clear vector database from disk: {str(e)}\")\n",
    "        \n",
    "\"\"\"\n",
    "å‚æ•°ç»„åˆ\tæ‰§è¡Œè¡Œä¸º\n",
    "by_id=False\tå¿½ç•¥ vector_db_idï¼Œæ‰¹é‡åˆ é™¤æ‰€æœ‰ UUID æ ¼å¼å‘é‡åº“æ–‡ä»¶å¤¹\n",
    "by_id=False, vector_db_id=\"xxx\"\tä»æ‰¹é‡åˆ é™¤æ‰€æœ‰ï¼Œä¼ å…¥çš„ ID è¢«å¿½ç•¥\n",
    "by_id=True\tå¿…é¡»ä¼  vector_db_idï¼Œå¦åˆ™æŠ¥é”™\n",
    "by_id=True, vector_db_id=\"xxx\"\tä»…åˆ é™¤æŒ‡å®š ID çš„å‘é‡åº“\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677c23ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. åˆå§‹åŒ–å·¥å…·ç±»\n",
    "rag_tool = RAGTool(\n",
    "    embedding_model=\"text-embedding-v3\",\n",
    "    embedding_api_key=\"your-api-key\",\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=200,\n",
    "    extract_image=False\n",
    ")\n",
    "\n",
    "# 2. è¯»å–PDFæ–‡ä»¶å¹¶åˆ†å—\n",
    "f= open(\"./test2.pdf\", \"rb\")\n",
    "file_content = f.read() \n",
    "f.close()\n",
    "chunks = rag_tool.process_pdf(\n",
    "    file_content=file_content, \n",
    "    filename=\"test.pdf\"\n",
    ")\n",
    "print(f\"PDFåˆ†å—å®Œæˆ, å…±{len(chunks)}å—\")\n",
    "\n",
    "# 3. æ„å»ºFAISSå‘é‡åº“å¹¶æ£€ç´¢çŸ¥è¯†\n",
    "vector_db = rag_tool.build_vector_db()\n",
    "knowledge_base_id = vector_db.id\n",
    "rag_tool.save_vector_db(folder_path=f\"./{knowledge_base_id}\", index_name=f\"{knowledge_base_id}\") # æ ¹æ®idæŸ¥è¯¢å¿…é¡»: ä¿å­˜å‘é‡æ•°æ®åº“åˆ°æœ¬åœ°\n",
    "rag_tool.load_vector_db(id=knowledge_base_id) # å…ˆåŠ è½½\n",
    "results = rag_tool.semantic_search(query=\"å¦‚ä½•æµ‹é‡ç»†ä¸ç›´å¾„ï¼Ÿ\", top_k=5) # å†æŸ¥è¯¢\n",
    "print(results, type(results))\n",
    "for i, res in enumerate(results, 1):\n",
    "    print(f\"ã€ç»“æœ{i}ã€‘\")\n",
    "    print(f\"ç›¸ä¼¼åº¦ï¼š{res['similarity_score']}\")\n",
    "    print(f\"å†…å®¹ï¼š{res['content'][:10]}...\")\n",
    "    print(f\"æ‰€å±æ–‡ä»¶ï¼š{res['metadata']['filename']}\")\n",
    "print(\"Tokenæ¶ˆè€—æƒ…å†µ: \", rag_tool.get_token_stats())\n",
    "rag_tool.clear()\n",
    "# rag_tool.clear_disk()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808436bc",
   "metadata": {},
   "source": [
    "### æ­¥éª¤ 1ï¼šæ›´æ–°ç³»ç»Ÿå¹¶å®‰è£… Redis\n",
    "&emsp;&emsp;é¦–å…ˆåœ¨ WSL ç»ˆç«¯æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š\n",
    "- 1. æ›´æ–°ç³»ç»ŸåŒ…åˆ—è¡¨\n",
    "```bash\n",
    "sudo apt update && sudo apt upgrade -y\n",
    "```\n",
    "\n",
    "- 2. å®‰è£… Redis æœåŠ¡å™¨\n",
    "```bash\n",
    "sudo apt install redis-server -y\n",
    "``` \n",
    "\n",
    "- 3. éªŒè¯å®‰è£…ï¼ˆæŸ¥çœ‹ç‰ˆæœ¬ï¼‰\n",
    "```bash\n",
    "redis-server --version\n",
    "```\n",
    "å®‰è£…æˆåŠŸåä¼šè¾“å‡ºç±»ä¼¼ï¼šRedis server v=6.0.16 sha=00000000:0 malloc=jemalloc-5.2.1 bits=64 build=abc1234\n",
    "\n",
    "### æ­¥éª¤ 2ï¼šé…ç½® Redisï¼ˆå…³é”®ï¼‰\n",
    "&emsp;&emsp;é»˜è®¤é…ç½®å¯èƒ½é™åˆ¶è¿œç¨‹è®¿é—® / æŒä¹…åŒ–ï¼Œéœ€è¦ä¿®æ”¹é…ç½®æ–‡ä»¶ï¼š\n",
    "- 2.1 ç¼–è¾‘ Redis é…ç½®æ–‡ä»¶\n",
    "```bash\n",
    "sudo nano /etc/redis/redis.conf\n",
    "```\n",
    "- 2.2 ä¿®æ”¹æ ¸å¿ƒé…ç½®é¡¹ï¼ˆæŒ‰éœ€æ±‚è°ƒæ•´ï¼‰\n",
    "&emsp;&emsp;åœ¨é…ç½®æ–‡ä»¶ä¸­æ‰¾åˆ°ä»¥ä¸‹é¡¹ï¼Œä¿®æ”¹ä¸ºå¯¹åº”å€¼ï¼ˆç”¨ Ctrl+W æœç´¢å…³é”®è¯ï¼‰ï¼š\n",
    "\n",
    "| é…ç½®é¡¹          | é»˜è®¤å€¼          | ä¿®æ”¹å            | è¯´æ˜                                                 |\n",
    "|-----------------|-----------------|-------------------|------------------------------------------------------|\n",
    "| bind            | 127.0.0.1 ::1   | 0.0.0.0           | å…è®¸æ‰€æœ‰ IP è®¿é—®ï¼ˆå¼€å‘ç¯å¢ƒï¼‰ï¼Œç”Ÿäº§ç¯å¢ƒéœ€æŒ‡å®šå…·ä½“ IP   |\n",
    "| protected-mode  | yes             | no                | å…³é—­ä¿æŠ¤æ¨¡å¼ï¼ˆå…è®¸å¤–éƒ¨è¿æ¥ï¼‰                         |\n",
    "| requirepass     | æ—               | your_redis_password | ï¼ˆå¯é€‰ï¼‰è®¾ç½® Redis å¯†ç ï¼Œå¯¹åº”ä»£ç ä¸­çš„ password      |\n",
    "| daemonize       | yes             | yes               | åå°è¿è¡Œï¼ˆä¿æŒé»˜è®¤å³å¯ï¼‰                             |\n",
    "| save            | 900 1           | ä¿ç•™é»˜è®¤          | æŒä¹…åŒ–ç­–ç•¥ï¼ˆè‡ªåŠ¨ä¿å­˜æ•°æ®åˆ°ç£ç›˜ï¼‰                     |\n",
    "\n",
    "ç¤ºä¾‹ä¿®æ”¹ï¼š\n",
    "```ini\n",
    "# å…è®¸æ‰€æœ‰IPè®¿é—®\n",
    "bind 0.0.0.0\n",
    "\n",
    "# å…³é—­ä¿æŠ¤æ¨¡å¼\n",
    "protected-mode no\n",
    "\n",
    "# ï¼ˆå¯é€‰ï¼‰è®¾ç½®å¯†ç ï¼ˆæ›¿æ¢ä¸ºä½ çš„å¯†ç ï¼‰\n",
    "requirepass my_redis_123456\n",
    "```\n",
    "\n",
    "- 2.3 ä¿å­˜å¹¶é€€å‡º\n",
    "æŒ‰ Ctrl+O ä¿å­˜ â†’ æŒ‰ Enter ç¡®è®¤ â†’ æŒ‰ Ctrl+X é€€å‡º nano ç¼–è¾‘å™¨ã€‚\n",
    "æ­¥éª¤ 3ï¼šå¯åŠ¨ / é‡å¯ Redis æœåŠ¡\n",
    "bash\n",
    "è¿è¡Œ\n",
    "é‡å¯ Redis æœåŠ¡ï¼ˆä½¿é…ç½®ç”Ÿæ•ˆï¼‰\n",
    "```bash\n",
    "sudo systemctl restart redis-server\n",
    "```\n",
    "\n",
    "# è®¾ç½®å¼€æœºè‡ªå¯ï¼ˆå¯é€‰ï¼‰\n",
    "```bash\n",
    "sudo systemctl enable redis-server\n",
    "```\n",
    "# æŸ¥çœ‹ Redis çŠ¶æ€\n",
    "```bash\n",
    "sudo systemctl status redis-server\n",
    "```\n",
    "âœ… æ­£å¸¸çŠ¶æ€ä¼šæ˜¾ç¤ºï¼šactive (running)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfc6aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Testing Redis Connection =====\n",
      "âœ… Redis connection successful!\n",
      "âœ… Test data read: rag:test:connection = redis_connection_ok\n",
      "âœ… Test data deleted: rag:test:connection\n",
      "\n",
      "ğŸ“Œ Redis Server Info:\n",
      "   Version: 7.0.15\n",
      "   Mode: standalone\n",
      "   Uptime: 24 seconds\n"
     ]
    }
   ],
   "source": [
    "# test redis \n",
    "import redis\n",
    "\n",
    "# ===================== é…ç½®é¡¹ï¼ˆæ›¿æ¢ä¸ºä½ çš„ WSL Redis ä¿¡æ¯ï¼‰ =====================\n",
    "REDIS_HOST = \"localhost\"  # WSL IPï¼ˆå¦‚ 172.17.0.2ï¼‰ï¼Œæœ¬åœ°æµ‹è¯•å¡« localhost å³å¯\n",
    "REDIS_PORT = 6379         # Redis é»˜è®¤ç«¯å£\n",
    "REDIS_PASSWORD = \"your-redis-password\"       # ä½ è®¾ç½®çš„ Redis å¯†ç ï¼ˆæ— åˆ™ç•™ç©ºï¼‰\n",
    "REDIS_DB = 0              # ä½¿ç”¨çš„æ•°æ®åº“ç¼–å·\n",
    "\n",
    "def test_redis_connection():\n",
    "    \"\"\"Test Redis connection and basic operations\"\"\"\n",
    "    try:\n",
    "        # 1. åˆå§‹åŒ– Redis å®¢æˆ·ç«¯ï¼ˆä¸ RAG API é…ç½®ä¸€è‡´ï¼‰\n",
    "        redis_client = redis.Redis(\n",
    "            host=REDIS_HOST,\n",
    "            port=REDIS_PORT,\n",
    "            db=REDIS_DB,\n",
    "            password=REDIS_PASSWORD,\n",
    "            decode_responses=True,  # ä¸ API ä¿æŒä¸€è‡´çš„é…ç½®\n",
    "            socket_timeout=5        # è¶…æ—¶æ—¶é—´ï¼Œé¿å…å¡æ­»\n",
    "        )\n",
    "        \n",
    "        # 2. æµ‹è¯•è¿æ¥ï¼ˆæ ¸å¿ƒéªŒè¯ï¼‰\n",
    "        pong = redis_client.ping()\n",
    "        if pong:\n",
    "            print(\"âœ… Redis connection successful!\")\n",
    "        else:\n",
    "            print(\"âŒ Redis ping failed!\")\n",
    "            return False\n",
    "        \n",
    "        # 3. æµ‹è¯•åŸºç¡€æ“ä½œï¼ˆæ¨¡æ‹Ÿ API ä¸­çš„æ•°æ®è¯»å†™ï¼‰\n",
    "        # å†™å…¥æµ‹è¯•æ•°æ®\n",
    "        test_key = \"rag:test:connection\"\n",
    "        redis_client.set(test_key, \"redis_connection_ok\", ex=300)  # 5åˆ†é’Ÿè¿‡æœŸ\n",
    "        \n",
    "        # è¯»å–æµ‹è¯•æ•°æ®\n",
    "        test_value = redis_client.get(test_key)\n",
    "        print(f\"âœ… Test data read: {test_key} = {test_value}\")\n",
    "        \n",
    "        # åˆ é™¤æµ‹è¯•æ•°æ®\n",
    "        redis_client.delete(test_key)\n",
    "        print(f\"âœ… Test data deleted: {test_key}\")\n",
    "        \n",
    "        # 4. è¾“å‡º Redis æœåŠ¡å™¨ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰\n",
    "        redis_info = redis_client.info(\"server\")\n",
    "        print(f\"\\nğŸ“Œ Redis Server Info:\")\n",
    "        print(f\"   Version: {redis_info.get('redis_version')}\")\n",
    "        print(f\"   Mode: {redis_info.get('redis_mode')}\")\n",
    "        print(f\"   Uptime: {redis_info.get('uptime_in_seconds')} seconds\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    except redis.exceptions.ConnectionError:\n",
    "        print(\"âŒ Connection Error: Could not connect to Redis server\")\n",
    "        print(\"   Check if Redis is running on WSL, and host/port is correct\")\n",
    "        return False\n",
    "    except redis.exceptions.AuthenticationError:\n",
    "        print(\"âŒ Authentication Error: Wrong Redis password\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Unexpected error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"===== Testing Redis Connection =====\")\n",
    "    test_redis_connection()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
